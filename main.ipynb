{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>1x1</th>\n",
       "      <th>1x2</th>\n",
       "      <th>1x3</th>\n",
       "      <th>1x4</th>\n",
       "      <th>1x5</th>\n",
       "      <th>1x6</th>\n",
       "      <th>1x7</th>\n",
       "      <th>1x8</th>\n",
       "      <th>1x9</th>\n",
       "      <th>...</th>\n",
       "      <th>28x19</th>\n",
       "      <th>28x20</th>\n",
       "      <th>28x21</th>\n",
       "      <th>28x22</th>\n",
       "      <th>28x23</th>\n",
       "      <th>28x24</th>\n",
       "      <th>28x25</th>\n",
       "      <th>28x26</th>\n",
       "      <th>28x27</th>\n",
       "      <th>28x28</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 785 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   label  1x1  1x2  1x3  1x4  1x5  1x6  1x7  1x8  1x9  ...  28x19  28x20  \\\n",
       "0      7    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "1      2    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "2      1    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "3      0    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "4      4    0    0    0    0    0    0    0    0    0  ...      0      0   \n",
       "\n",
       "   28x21  28x22  28x23  28x24  28x25  28x26  28x27  28x28  \n",
       "0      0      0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0      0      0  \n",
       "4      0      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 785 columns]"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "data = pd.read_csv('mnist_test.csv')\n",
    "data.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def sigmoid(z):\n",
    "#     return 1 / (1+np.exp(-z))\n",
    "\n",
    "# class LossFunction:\n",
    "#     def __init__(self, X, y):\n",
    "#         self.X = X\n",
    "#         self.y = y\n",
    "    \n",
    "#     def loss(self, a : np.ndarray):\n",
    "#         prob = sigmoid(np.dot(self.X, a))\n",
    "#         values = - self.y * np.log(prob) - (1 - self.y) * np.log(1 - prob)\n",
    "#         return np.nansum(values) / self.y.shape[0]\n",
    "\n",
    "#     def gradient(self, a : np.ndarray):\n",
    "#         prob = sigmoid(np.dot(self.X, a))\n",
    "#         sub_coefficient = -(self.y - prob) \n",
    "#         return np.dot(self.X.T, sub_coefficient) / self.y.shape[0]\n",
    "    \n",
    "#     def precision(self, a : np.ndarray):\n",
    "#         prob = sigmoid(np.dot(self.X, a))\n",
    "#         prob = np.array(prob >= 0.5, dtype=np.int32)\n",
    "#         return np.sum(prob == self.y) / self.y.shape[0]\n",
    "\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 785), (10000,))"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = data.drop('label', axis=1).values\n",
    "# X: shape m x 784\n",
    "#append a column of 1s\n",
    "X = np.concatenate((np.ones((X.shape[0], 1)), X), axis=1)\n",
    "y = data['label'].values\n",
    "X.shape, y.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_functions = []\n",
    "for digit in range(10):\n",
    "    y_digit = np.array(y == digit, dtype=np.int32)\n",
    "    loss_functions.append(LossFunction(X, y_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(loss_func, starting_point, learning_rate = 0.00001, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient(cur_point)\n",
    "        # print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss(cur_point), loss_func.precision(cur_point)))\n",
    "        cur_point = cur_point - learning_rate * grad\n",
    "        if np.linalg.norm(grad) < precision:\n",
    "            break\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for digit 0: 0.9875\n",
      "Accuracy for digit 1: 0.9889\n",
      "Accuracy for digit 2: 0.9711\n",
      "Accuracy for digit 3: 0.9666\n",
      "Accuracy for digit 4: 0.972\n",
      "Accuracy for digit 5: 0.957\n",
      "Accuracy for digit 6: 0.9784\n",
      "Accuracy for digit 7: 0.9791\n",
      "Accuracy for digit 8: 0.9348\n",
      "Accuracy for digit 9: 0.949\n"
     ]
    }
   ],
   "source": [
    "optimal_points = []\n",
    "\n",
    "for digit in range(10):\n",
    "    optimal_point = gradient_descent(loss_functions[digit], np.zeros(X.shape[1]))\n",
    "    print(\"Accuracy for digit {}: {}\".format(digit, loss_functions[digit].precision(optimal_point)))\n",
    "    optimal_points.append(optimal_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(X, optimal_points):\n",
    "    prob = np.zeros((X.shape[0], 10))\n",
    "    for digit in range(10):\n",
    "        prob[:, digit] = sigmoid(np.dot(X, optimal_points[digit]))\n",
    "    return np.argmax(prob, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def draw_digit(data, row_id):\n",
    "    row = data.iloc[row_id]\n",
    "    label = row['label']\n",
    "    image = row.drop('label').values.reshape(28, 28)\n",
    "    plt.title('Digit Label = {}'.format(label))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_csv('mnist_test.csv')\n",
    "X_test = data_test.drop('label', axis=1).values\n",
    "X_test = np.concatenate((np.ones((X_test.shape[0], 1)), X_test), axis=1)\n",
    "y_test = data_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "draw_digit(data_test, 2005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4], dtype=int64)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "infer(np.array([X_test[2005]]), optimal_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8819"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate(X, y, optimal_points):\n",
    "    y_pred = infer(X, optimal_points)\n",
    "    return np.sum(y_pred == y) / y.shape[0]\n",
    "evaluate(X_test, y_test, optimal_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22.957171356733"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Immplementing the stable váº»sion of softmax function\n",
    "def softmax(z):\n",
    "    return np.exp(z-np.max(z, axis=0)) / np.sum(np.exp(z-np.max(z, axis=0)), axis=0)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        # Create one-hot encoding from vector y\n",
    "        self.y = np.zeros((y.shape[0], 10))\n",
    "        self.y[np.arange(y.shape[0]), y] = 1\n",
    "\n",
    "    def loss_softmax(self, a: np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        #avoid inf\n",
    "        prob = np.clip(prob, 1e-10, 1)\n",
    "        values = -np.sum(self.y* np.log(prob))  \n",
    "        return values / self.y.shape[0]\n",
    "    \n",
    "    def gradient(self, a: np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        sub_coefficient = prob - self.y\n",
    "        return np.dot(self.X.T, sub_coefficient) / self.y.shape[0]\n",
    "    \n",
    "    def precision(self, a:np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        prob = np.argmax(prob, axis = 0)\n",
    "        return np.sum(prob == self.y) / self.y.shape[0]\n",
    "    \n",
    "loss_func = LogisticRegression(X, y)\n",
    "    #Randomly initialize a, each has real value from normal distribution between -0.01 and 0.01\n",
    "a = np.random.randn(X.shape[1], 10) * 0.01  \n",
    "loss_func.loss_softmax(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam optimizer\n",
    "def adam(loss_func, starting_point, num_steps = 40, precision = 0.00001):\n",
    "    cur_point = starting_point\n",
    "    m = np.zeros(cur_point.shape)\n",
    "    v = np.zeros(cur_point.shape)\n",
    "    beta1 = 0.9\n",
    "    beta2 = 0.999\n",
    "    epsilon = 1e-8\n",
    "    for i in range(num_steps):\n",
    "        gra = loss_func.gradient(cur_point)\n",
    "        m = beta1 * m + (1 - beta1) * gra\n",
    "        v = beta2 * v + (1 - beta2) * gra**2\n",
    "        m_hat = m / (1 - beta1**(i+1))\n",
    "        v_hat = v / (1 - beta2**(i+1))\n",
    "        cur_point = cur_point - 0.001 * m_hat / (np.sqrt(v_hat) + epsilon)\n",
    "        if np.linalg.norm(gra) < precision:\n",
    "            break\n",
    "        print(\"Iteration {}: loss = {}, precision = {}\".format(i, loss_func.loss_softmax(cur_point), loss_func.precision(cur_point)))\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: loss = 22.979595051478857, precision = 0.0\n",
      "Iteration 1: loss = 22.997014585786367, precision = 0.0\n",
      "Iteration 2: loss = 23.00139603337103, precision = 0.0\n",
      "Iteration 3: loss = 23.00630407741307, precision = 0.0\n",
      "Iteration 4: loss = 23.014016676808044, precision = 0.0\n",
      "Iteration 5: loss = 23.01667316630541, precision = 0.0\n",
      "Iteration 6: loss = 23.017309276795526, precision = 0.0\n",
      "Iteration 7: loss = 23.01792576385205, precision = 0.0\n",
      "Iteration 8: loss = 23.01886512333145, precision = 0.0\n",
      "Iteration 9: loss = 23.01979908267686, precision = 0.0\n",
      "Iteration 10: loss = 23.02049441455881, precision = 0.0\n",
      "Iteration 11: loss = 23.020715852866285, precision = 0.0\n",
      "Iteration 12: loss = 23.020937259540663, precision = 0.0\n",
      "Iteration 13: loss = 23.02115864367981, precision = 0.0\n",
      "Iteration 14: loss = 23.021245759754542, precision = 0.0\n",
      "Iteration 15: loss = 23.02124575975459, precision = 0.0\n",
      "Iteration 16: loss = 23.021245759754674, precision = 0.0\n",
      "Iteration 17: loss = 23.021245759754816, precision = 0.0\n",
      "Iteration 18: loss = 23.02124575975506, precision = 0.0\n",
      "Iteration 19: loss = 23.021245759755473, precision = 0.0\n",
      "Iteration 20: loss = 23.02124575975617, precision = 0.0\n",
      "Iteration 21: loss = 23.021245759757353, precision = 0.0\n",
      "Iteration 22: loss = 23.021245759759353, precision = 0.0\n",
      "Iteration 23: loss = 23.02124575976274, precision = 0.0\n",
      "Iteration 24: loss = 23.021245759768465, precision = 0.0\n",
      "Iteration 25: loss = 23.021245759778157, precision = 0.0\n",
      "Iteration 26: loss = 23.02124575979454, precision = 0.0\n",
      "Iteration 27: loss = 23.021245759822236, precision = 0.0\n",
      "Iteration 28: loss = 23.021245759869043, precision = 0.0\n",
      "Iteration 29: loss = 23.02124575994813, precision = 0.0\n",
      "Iteration 30: loss = 23.02124576008172, precision = 0.0\n",
      "Iteration 31: loss = 23.021245760307302, precision = 0.0\n",
      "Iteration 32: loss = 23.021245760688128, precision = 0.0\n",
      "Iteration 33: loss = 23.021245761330864, precision = 0.0\n",
      "Iteration 34: loss = 23.021245762415347, precision = 0.0\n",
      "Iteration 35: loss = 23.0212457642447, precision = 0.0\n",
      "Iteration 36: loss = 23.02124576732971, precision = 0.0\n",
      "Iteration 37: loss = 23.021245772530847, precision = 0.0\n",
      "Iteration 38: loss = 23.021245781297207, precision = 0.0\n",
      "Iteration 39: loss = 23.021245796068264, precision = 0.0\n",
      "Iteration 40: loss = 23.021245820948774, precision = 0.0\n",
      "Iteration 41: loss = 23.02124586284127, precision = 0.0\n",
      "Iteration 42: loss = 23.021245933342268, precision = 0.0\n",
      "Iteration 43: loss = 23.02124605190724, precision = 0.0\n",
      "Iteration 44: loss = 23.02124625110458, precision = 0.0\n",
      "Iteration 45: loss = 23.02124658525928, precision = 0.0\n",
      "Iteration 46: loss = 23.02124714445707, precision = 0.0\n",
      "Iteration 47: loss = 23.021248076642976, precision = 0.0\n",
      "Iteration 48: loss = 23.021249620907042, precision = 0.0\n",
      "Iteration 49: loss = 23.021252153483978, precision = 0.0\n",
      "Iteration 50: loss = 23.02125624098903, precision = 0.0\n",
      "Iteration 51: loss = 23.02126267747221, precision = 0.0\n",
      "Iteration 52: loss = 23.021272451796698, precision = 0.0\n",
      "Iteration 53: loss = 23.021286572933693, precision = 0.0\n",
      "Iteration 54: loss = 23.02130573661303, precision = 0.0\n",
      "Iteration 55: loss = 23.02132997938136, precision = 0.0\n",
      "Iteration 56: loss = 23.021358580233493, precision = 0.0\n",
      "Iteration 57: loss = 23.021390310870284, precision = 0.0\n",
      "Iteration 58: loss = 23.021423841297217, precision = 0.0\n",
      "Iteration 59: loss = 23.021458043540235, precision = 0.0\n",
      "Iteration 60: loss = 23.021492105813707, precision = 0.0\n",
      "Iteration 61: loss = 23.021525514197837, precision = 0.0\n",
      "Iteration 62: loss = 23.021557983215484, precision = 0.0\n",
      "Iteration 63: loss = 23.021589384063102, precision = 0.0\n",
      "Iteration 64: loss = 23.02161968804645, precision = 0.0\n",
      "Iteration 65: loss = 23.021648926984764, precision = 0.0\n",
      "Iteration 66: loss = 23.021677167137945, precision = 0.0\n",
      "Iteration 67: loss = 23.021704492625013, precision = 0.0\n",
      "Iteration 68: loss = 23.021730995119583, precision = 0.0\n",
      "Iteration 69: loss = 23.021756767577173, precision = 0.0\n",
      "Iteration 70: loss = 23.021781900515766, precision = 0.0\n",
      "Iteration 71: loss = 23.02180647990186, precision = 0.0\n",
      "Iteration 72: loss = 23.02183058604171, precision = 0.0\n",
      "Iteration 73: loss = 23.02185429309826, precision = 0.0\n",
      "Iteration 74: loss = 23.021877668993852, precision = 0.0\n",
      "Iteration 75: loss = 23.021900775546356, precision = 0.0\n",
      "Iteration 76: loss = 23.021923668741834, precision = 0.0\n",
      "Iteration 77: loss = 23.021946399081887, precision = 0.0\n",
      "Iteration 78: loss = 23.02196901196639, precision = 0.0\n",
      "Iteration 79: loss = 23.02199154808671, precision = 0.0\n",
      "Iteration 80: loss = 23.02201404381395, precision = 0.0\n",
      "Iteration 81: loss = 23.022036531572745, precision = 0.0\n",
      "Iteration 82: loss = 23.022059040195256, precision = 0.0\n",
      "Iteration 83: loss = 23.022081595252466, precision = 0.0\n",
      "Iteration 84: loss = 23.022104219361637, precision = 0.0\n",
      "Iteration 85: loss = 23.022126932469924, precision = 0.0\n",
      "Iteration 86: loss = 23.022149752114654, precision = 0.0\n",
      "Iteration 87: loss = 23.022172693661435, precision = 0.0\n",
      "Iteration 88: loss = 23.02219577052121, precision = 0.0\n",
      "Iteration 89: loss = 23.02221899434782, precision = 0.0\n",
      "Iteration 90: loss = 23.022242375217324, precision = 0.0\n",
      "Iteration 91: loss = 23.02226592179061, precision = 0.0\n",
      "Iteration 92: loss = 23.022289641460635, precision = 0.0\n",
      "Iteration 93: loss = 23.022313540485552, precision = 0.0\n",
      "Iteration 94: loss = 23.022337624109003, precision = 0.0\n",
      "Iteration 95: loss = 23.02236189666872, precision = 0.0\n",
      "Iteration 96: loss = 23.022386361694423, precision = 0.0\n",
      "Iteration 97: loss = 23.022411021996113, precision = 0.0\n",
      "Iteration 98: loss = 23.022435879743547, precision = 0.0\n",
      "Iteration 99: loss = 23.022460936537787, precision = 0.0\n",
      "Iteration 100: loss = 23.022486193475565, precision = 0.0\n",
      "Iteration 101: loss = 23.02251165120711, precision = 0.0\n",
      "Iteration 102: loss = 23.02253730998815, precision = 0.0\n",
      "Iteration 103: loss = 23.022563169726535, precision = 0.0\n",
      "Iteration 104: loss = 23.022589230024135, precision = 0.0\n",
      "Iteration 105: loss = 23.02261549021437, precision = 0.0\n",
      "Iteration 106: loss = 23.02264194939584, precision = 0.0\n",
      "Iteration 107: loss = 23.022668606462478, precision = 0.0\n",
      "Iteration 108: loss = 23.022695460130485, precision = 0.0\n",
      "Iteration 109: loss = 23.02272250896246, precision = 0.0\n",
      "Iteration 110: loss = 23.02274975138893, precision = 0.0\n",
      "Iteration 111: loss = 23.022777185727588, precision = 0.0\n",
      "Iteration 112: loss = 23.02280481020044, precision = 0.0\n",
      "Iteration 113: loss = 23.022832622949114, precision = 0.0\n",
      "Iteration 114: loss = 23.022860622048448, precision = 0.0\n",
      "Iteration 115: loss = 23.02288880551861, precision = 0.0\n",
      "Iteration 116: loss = 23.02291717133583, precision = 0.0\n",
      "Iteration 117: loss = 23.02294571744196, precision = 0.0\n",
      "Iteration 118: loss = 23.022974441752883, precision = 0.0\n",
      "Iteration 119: loss = 23.02300334216601, precision = 0.0\n",
      "Iteration 120: loss = 23.023032416566856, precision = 0.0\n",
      "Iteration 121: loss = 23.023061662834852, precision = 0.0\n",
      "Iteration 122: loss = 23.023091078848438, precision = 0.0\n",
      "Iteration 123: loss = 23.023120662489546, precision = 0.0\n",
      "Iteration 124: loss = 23.023150411647503, precision = 0.0\n",
      "Iteration 125: loss = 23.023180324222444, precision = 0.0\n",
      "Iteration 126: loss = 23.023210398128267, precision = 0.0\n",
      "Iteration 127: loss = 23.023240631295195, precision = 0.0\n",
      "Iteration 128: loss = 23.023271021671956, precision = 0.0\n",
      "Iteration 129: loss = 23.023301567227698, precision = 0.0\n",
      "Iteration 130: loss = 23.023332265953545, precision = 0.0\n",
      "Iteration 131: loss = 23.02336311586397, precision = 0.0\n",
      "Iteration 132: loss = 23.02339411499789, precision = 0.0\n",
      "Iteration 133: loss = 23.023425261419607, precision = 0.0\n",
      "Iteration 134: loss = 23.023456553219535, precision = 0.0\n",
      "Iteration 135: loss = 23.02348798851479, precision = 0.0\n",
      "Iteration 136: loss = 23.023519565449636, precision = 0.0\n",
      "Iteration 137: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 138: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 139: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 140: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 141: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 142: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 143: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 144: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 145: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 146: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 147: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 148: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 149: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 150: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 151: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 152: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 153: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 154: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 155: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 156: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 157: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 158: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 159: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 160: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 161: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 162: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 163: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 164: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 165: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 166: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 167: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 168: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 169: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 170: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 171: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 172: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 173: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 174: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 175: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 176: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 177: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 178: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 179: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 180: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 181: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 182: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 183: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 184: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 185: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 186: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 187: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 188: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 189: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 190: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 191: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 192: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 193: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 194: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 195: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 196: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 197: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 198: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 199: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 200: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 201: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 202: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 203: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 204: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 205: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 206: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 207: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 208: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 209: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 210: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 211: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 212: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 213: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 214: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 215: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 216: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 217: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 218: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 219: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 220: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 221: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 222: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 223: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 224: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 225: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 226: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 227: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 228: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 229: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 230: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 231: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 232: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 233: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 234: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 235: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 236: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 237: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 238: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 239: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 240: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 241: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 242: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 243: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 244: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 245: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 246: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 247: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 248: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 249: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 250: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 251: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 252: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 253: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 254: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 255: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 256: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 257: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 258: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 259: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 260: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 261: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 262: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 263: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 264: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 265: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 266: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 267: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 268: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 269: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 270: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 271: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 272: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 273: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 274: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 275: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 276: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 277: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 278: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 279: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 280: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 281: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 282: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 283: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 284: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 285: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 286: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 287: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 288: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 289: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 290: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 291: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 292: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 293: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 294: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 295: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 296: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 297: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 298: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 299: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 300: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 301: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 302: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 303: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 304: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 305: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 306: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 307: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 308: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 309: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 310: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 311: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 312: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 313: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 314: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 315: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 316: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 317: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 318: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 319: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 320: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 321: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 322: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 323: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 324: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 325: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 326: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 327: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 328: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 329: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 330: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 331: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 332: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 333: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 334: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 335: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 336: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 337: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 338: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 339: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 340: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 341: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 342: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 343: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 344: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 345: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 346: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 347: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 348: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 349: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 350: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 351: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 352: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 353: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 354: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 355: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 356: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 357: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 358: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 359: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 360: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 361: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 362: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 363: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 364: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 365: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 366: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 367: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 368: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 369: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 370: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 371: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 372: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 373: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 374: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 375: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 376: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 377: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 378: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 379: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 380: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 381: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 382: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 383: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 384: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 385: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 386: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 387: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 388: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 389: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 390: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 391: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 392: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 393: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 394: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 395: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 396: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 397: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 398: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 399: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 400: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 401: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 402: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 403: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 404: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 405: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 406: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 407: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 408: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 409: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 410: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 411: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 412: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 413: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 414: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 415: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 416: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 417: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 418: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 419: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 420: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 421: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 422: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 423: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 424: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 425: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 426: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 427: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 428: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 429: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 430: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 431: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 432: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 433: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 434: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 435: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 436: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 437: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 438: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 439: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 440: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 441: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 442: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 443: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 444: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 445: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 446: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 447: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 448: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 449: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 450: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 451: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 452: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 453: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 454: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 455: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 456: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 457: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 458: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 459: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 460: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 461: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 462: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 463: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 464: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 465: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 466: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 467: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 468: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 469: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 470: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 471: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 472: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 473: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 474: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 475: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 476: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 477: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 478: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 479: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 480: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 481: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 482: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 483: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 484: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 485: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 486: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 487: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 488: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 489: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 490: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 491: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 492: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 493: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 494: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 495: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 496: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 497: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 498: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 499: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 500: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 501: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 502: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 503: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 504: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 505: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 506: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 507: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 508: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 509: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 510: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 511: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 512: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 513: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 514: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 515: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 516: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 517: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 518: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 519: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 520: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 521: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 522: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 523: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 524: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 525: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 526: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 527: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 528: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 529: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 530: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 531: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 532: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 533: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 534: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 535: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 536: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 537: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 538: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 539: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 540: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 541: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 542: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 543: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 544: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 545: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 546: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 547: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 548: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 549: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 550: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 551: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 552: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 553: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 554: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 555: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 556: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 557: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 558: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 559: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 560: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 561: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 562: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 563: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 564: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 565: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 566: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 567: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 568: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 569: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 570: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 571: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 572: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 573: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 574: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 575: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 576: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 577: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 578: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 579: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 580: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 581: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 582: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 583: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 584: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 585: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 586: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 587: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 588: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 589: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 590: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 591: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 592: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 593: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 594: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 595: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 596: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 597: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 598: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 599: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 600: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 601: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 602: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 603: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 604: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 605: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 606: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 607: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 608: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 609: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 610: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 611: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 612: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 613: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 614: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 615: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 616: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 617: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 618: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 619: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 620: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 621: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 622: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 623: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 624: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 625: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 626: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 627: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 628: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 629: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 630: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 631: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 632: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 633: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 634: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 635: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 636: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 637: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 638: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 639: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 640: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 641: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 642: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 643: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 644: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 645: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 646: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 647: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 648: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 649: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 650: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 651: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 652: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 653: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 654: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 655: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 656: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 657: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 658: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 659: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 660: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 661: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 662: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 663: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 664: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 665: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 666: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 667: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 668: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 669: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 670: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 671: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 672: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 673: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 674: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 675: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 676: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 677: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 678: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 679: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 680: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 681: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 682: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 683: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 684: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 685: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 686: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 687: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 688: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 689: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 690: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 691: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 692: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 693: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 694: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 695: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 696: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 697: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 698: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 699: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 700: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 701: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 702: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 703: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 704: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 705: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 706: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 707: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 708: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 709: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 710: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 711: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 712: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 713: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 714: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 715: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 716: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 717: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 718: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 719: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 720: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 721: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 722: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 723: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 724: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 725: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 726: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 727: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 728: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 729: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 730: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 731: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 732: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 733: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 734: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 735: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 736: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 737: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 738: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 739: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 740: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 741: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 742: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 743: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 744: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 745: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 746: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 747: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 748: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 749: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 750: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 751: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 752: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 753: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 754: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 755: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 756: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 757: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 758: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 759: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 760: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 761: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 762: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 763: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 764: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 765: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 766: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 767: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 768: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 769: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 770: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 771: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 772: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 773: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 774: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 775: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 776: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 777: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 778: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 779: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 780: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 781: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 782: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 783: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 784: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 785: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 786: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 787: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 788: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 789: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 790: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 791: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 792: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 793: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 794: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 795: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 796: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 797: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 798: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 799: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 800: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 801: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 802: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 803: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 804: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 805: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 806: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 807: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 808: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 809: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 810: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 811: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 812: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 813: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 814: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 815: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 816: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 817: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 818: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 819: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 820: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 821: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 822: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 823: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 824: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 825: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 826: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 827: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 828: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 829: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 830: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 831: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 832: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 833: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 834: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 835: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 836: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 837: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 838: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 839: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 840: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 841: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 842: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 843: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 844: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 845: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 846: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 847: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 848: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 849: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 850: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 851: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 852: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 853: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 854: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 855: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 856: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 857: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 858: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 859: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 860: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 861: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 862: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 863: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 864: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 865: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 866: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 867: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 868: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 869: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 870: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 871: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 872: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 873: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 874: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 875: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 876: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 877: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 878: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 879: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 880: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 881: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 882: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 883: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 884: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 885: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 886: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 887: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 888: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 889: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 890: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 891: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 892: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 893: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 894: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 895: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 896: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 897: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 898: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 899: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 900: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 901: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 902: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 903: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 904: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 905: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 906: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 907: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 908: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 909: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 910: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 911: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 912: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 913: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 914: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 915: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 916: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 917: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 918: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 919: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 920: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 921: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 922: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 923: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 924: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 925: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 926: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 927: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 928: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 929: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 930: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 931: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 932: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 933: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 934: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 935: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 936: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 937: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 938: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 939: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 940: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 941: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 942: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 943: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 944: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 945: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 946: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 947: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 948: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 949: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 950: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 951: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 952: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 953: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 954: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 955: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 956: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 957: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 958: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 959: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 960: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 961: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 962: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 963: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 964: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 965: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 966: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 967: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 968: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 969: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 970: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 971: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 972: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 973: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 974: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 975: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 976: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 977: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 978: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 979: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 980: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 981: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 982: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 983: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 984: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 985: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 986: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 987: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 988: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 989: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 990: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 991: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 992: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 993: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 994: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 995: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 996: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 997: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 998: loss = 23.023548344847462, precision = 0.0\n",
      "Iteration 999: loss = 23.023548344847462, precision = 0.0\n"
     ]
    }
   ],
   "source": [
    "W = adam(loss_func, num_steps=1000, precision=0.00001, starting_point=a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement focal loss to deal with the imbalanced data\n",
    "class FocalLoss:\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def focal_loss(self, a:np.ndarray, gamma = 2, alpha = 0.25):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        values = - alpha * (1 - prob) ** gamma * np.log(prob)\n",
    "        return np.nansum(values) / self.y.shape[0]\n",
    "\n",
    "    def gradient_focal_loss(self, a:np.ndarray, gamma = 2, alpha = 0.25):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        sub_coefficient = - alpha * (1 - prob) ** gamma * (gamma * prob * np.log(prob) + prob - 1)\n",
    "        return np.dot(self.X.T, sub_coefficient) / self.y.shape[0]\n",
    "\n",
    "    def precision_focal_loss(self, a:np.ndarray):\n",
    "        prob = softmax(np.dot(self.X, a))\n",
    "        prob = np.argmax(prob, axis = 0)\n",
    "        return np.sum(prob == self.y) / self.y.shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "loss_functions = []\n",
    "for digit in range(10):\n",
    "    y_digit = np.array(y == digit, dtype=np.int32)\n",
    "    loss_functions.append(FocalLoss(X, y_digit))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_softmax_process(loss_func, starting_point, learning_rate = 0.00001, num_steps = 40, precision=0.00001):\n",
    "    cur_point = starting_point\n",
    "    for i in range(num_steps):\n",
    "        grad = loss_func.gradient_focal_loss(cur_point)\n",
    "        cur_point = cur_point - learning_rate * grad\n",
    "        if np.linalg.norm(grad) < precision:\n",
    "            break\n",
    "    return cur_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_points = []\n",
    "\n",
    "for digit in range(10):\n",
    "    optimal_point = gradient_softmax_process(loss_functions[digit], np.zeros(X.shape[1]))\n",
    "    print(\"Accuracy for digit {}: {}\".format(digit, loss_functions[digit].precision_focal_loss(optimal_point)))\n",
    "    optimal_points.append(optimal_point)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
